{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17f6ec64-ceaf-48f8-be68-07b88aafbb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous profile: profile_for_article\n",
      "Setting new profile to: profile_for_article\n",
      "Current iam_role is arn:aws:iam::405389362913:role/iamr-glueintsessionsdemo\n",
      "iam_role has been set to arn:aws:iam::405389362913:role/iamr-glueintsessionsdemo.\n",
      "Current idle_timeout is 10 minutes.\n",
      "idle_timeout has been set to 10 minutes.\n",
      "Previous number of workers: 2\n",
      "Setting new number of workers to: 2\n"
     ]
    }
   ],
   "source": [
    "%profile profile_for_article\n",
    "%iam_role arn:aws:iam::405389362913:role/iamr-glueintsessionsdemo\n",
    "%idle_timeout 10\n",
    "%number_of_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "314ccd0e-1c7b-4ddb-a113-439c4b017a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following configurations have been updated: {'--s3_bucket_name': 's3-glueintsessionsdemo-data'}\n"
     ]
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"--s3_bucket_name\" : \"s3-glueintsessionsdemo-data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6670c52d-50fa-4f8d-865d-aaba5ecc2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following configurations have been updated: {'--datalake-formats': 'iceberg', '--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.warehouse=s3://s3-glueintsessionsdemo-data/iceberg/ --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO'}\n"
     ]
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"--datalake-formats\" : \"iceberg\",\n",
    "    \"--conf\" : \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.warehouse=s3://s3-glueintsessionsdemo-data/iceberg/ --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ab311b-ca8e-4d50-b5a2-bff75f1e1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Session Type: etl\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: 63d3d32b-262a-4c7d-a5fd-074e4a9f3a4b\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 1.0.0\n",
      "--enable-glue-datacatalog true\n",
      "--S3_BUCKET_NAME s3-glueintsessionsdemo-data\n",
      "--datalake-formats iceberg\n",
      "--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.warehouse=s3://s3-glueintsessionsdemo-data/iceberg/ --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\n",
      "--s3_bucket_name s3-glueintsessionsdemo-data\n",
      "Waiting for session 63d3d32b-262a-4c7d-a5fd-074e4a9f3a4b to get into ready status...\n",
      "Session 63d3d32b-262a-4c7d-a5fd-074e4a9f3a4b has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485db5a4-f7f3-475e-8a56-b4f47f7d78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from awsglue.utils import getResolvedOptions\n",
    "parameter_name = 's3_bucket_name'\n",
    "args = getResolvedOptions(sys.argv,[parameter_name])\n",
    "s3_bucket_name = args[parameter_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef7c64b-b53e-4a55-b467-a5d5c73355b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Read dataframe directly from s3 object using the variable\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"s3://{s3_bucket_name}/in/lakes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9df456-23cf-412c-8f27-d56fc1f44414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert it into parquet format and writes to specified S3 path using the variable\n",
    "df.write.mode(\"overwrite\").parquet(f\"s3://{s3_bucket_name}/out/lakes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9001c7e-a0ab-4d08-acc9-1c214282a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare aggregated dataframe\n",
    "agg_df = df.groupBy(\"continent\").count().withColumnRenamed(\"count\", \"number_of_lakes\")\n",
    "\n",
    "# Outputs dataframe\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788af88f-0f8e-4ae7-a9be-5152541a14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write aggregated dataframe to Iceberg table\n",
    "agg_df.createOrReplaceTempView(\"tmp_lakes\")\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS glue_catalog.default.lakes_iceberg\n",
    "USING iceberg\n",
    "AS SELECT * FROM tmp_lakes\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560db8a-66e3-41af-aeee-5fe25cc931f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
