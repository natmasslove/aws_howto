{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6ec64-ceaf-48f8-be68-07b88aafbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%profile profile_for_article\n",
    "%iam_role arn:aws:iam::405389362913:role/iamr-glueintsessionsdemo\n",
    "%idle_timeout 10\n",
    "%number_of_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab311b-ca8e-4d50-b5a2-bff75f1e1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7c64b-b53e-4a55-b467-a5d5c73355b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the S3 bucket name variable\n",
    "s3_bucket_name = \"s3-glueintsessionsdemo-data\"\n",
    "\n",
    "# 1. Read dataframe directly from s3 object using the variable\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"s3://{s3_bucket_name}/in/lakes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9df456-23cf-412c-8f27-d56fc1f44414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert it into parquet format and writes to specified S3 path using the variable\n",
    "df.write.mode(\"overwrite\").parquet(f\"s3://{s3_bucket_name}/out/lakes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9001c7e-a0ab-4d08-acc9-1c214282a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare aggregated dataframe\n",
    "agg_df = df.groupBy(\"continent\").count().withColumnRenamed(\"count\", \"number_of_lakes\")\n",
    "\n",
    "# Write aggregated dataframe to another S3 path using the variable\n",
    "agg_df.write.mode(\"overwrite\").parquet(f\"s3://{s3_bucket_name}/out/lakes/aggregated/\")\n",
    "\n",
    "# Outputs dataframe\n",
    "agg_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560db8a-66e3-41af-aeee-5fe25cc931f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
